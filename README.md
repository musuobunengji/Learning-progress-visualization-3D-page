# ChapterGraph

**Retrieval-Oriented Chapter Relationship Analysis Pipeline (Python)**

ChapterGraph is a Python-based retrieval pipeline that analyzes relationships between chapters across multiple curated technical documents.

The project focuses on **turning semi-structured technical content into structured signals**, and on **designing a debuggable, multi-stage retrieval workflow** for discovering cross-document relationships.
Rather than optimizing for maximum semantic power, the system emphasizes **pipeline clarity, controllable complexity, and engineering realism**.

This repository is designed as an **internal engineering tool prototype**, with a clear evolution path toward embeddings, vector search, and service deployment.

---

## âœ¨ Core Capabilities

* **Deterministic document ingestion**
  Convert curated technical texts into a normalized, chapter-centric JSON schema.

* **Signal enrichment at chapter granularity**
  Aggregate chapter-level textual signals (sections, bullets) into a canonical representation used consistently across the pipeline.

* **Multi-stage retrieval workflow**
  Explicit separation between candidate generation and similarity scoring, following classical information retrieval design.

* **Inverted-indexâ€“based candidate pruning**
  Use lightweight lexical signals to aggressively reduce the comparison space before scoring.

* **Continuous similarity scoring**
  Apply TF-IDFâ€“based similarity to produce interpretable, continuous relevance scores instead of binary overlaps.

* **Modular, extensible architecture**
  Candidate generation, similarity scoring, and pipeline orchestration are abstracted to allow controlled evolution.

---

## ðŸ§± Pipeline Overview

```
Curated Technical Text
        â†“
Ingestion
        â†“
Structured Chapter JSON
        â†“
Signal Enrichment
        â†“
Chapter Text Normalization
        â†“
Candidate Generation (Inverted Index)
        â†“
Similarity Scoring (TF-IDF)
        â†“
Chapter Relationship Graph
```

The system follows a **classical multi-stage retrieval funnel**:

* **Early pruning for efficiency** (candidate generation)
* **Scoring for relevance** (similarity computation)
* **Designed for future semantic upgrades**, without coupling them prematurely to the core pipeline

---

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ book_content/
â”‚   â”œâ”€â”€ books.yaml                 # Document configuration
â”‚   â”œâ”€â”€ core_java_content.txt
â”‚   â”œâ”€â”€ spring_in_action_content.txt
â”‚   â””â”€â”€ spring_start_here_content.txt
â”‚
â”œâ”€â”€ feature_achievement/
â”‚   â”œâ”€â”€ ingestion/                 # Parsing & normalization
â”‚   â”œâ”€â”€ enrichment/                # Chapter-level signal construction
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ candidates/            # Candidate generation strategies
â”‚   â”‚   â”œâ”€â”€ similarity/            # Similarity scoring strategies
â”‚   â”‚   â”œâ”€â”€ utils/                 # Shared builders (TF-IDF, indices)
â”‚   â”‚   â”œâ”€â”€ pipeline.py            # RetrievalPipeline orchestration
â”‚   â”‚   â””â”€â”€ edge_generation.py     # Thin edge construction layer
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â””â”€â”€ README.md
```

The codebase is structured to clearly distinguish between:

* **Workflow orchestration**
* **Replaceable retrieval strategies**
* **Shared resource construction**

---

## â–¶ï¸ Running the Pipeline

> âš ï¸ **Assumption**
> Input documents are **manually curated technical texts**.
> Ingestion is intentionally conservative and not designed for arbitrary raw files.

### 1. Configure documents

Edit `book_content/books.yaml`:

```yaml
- book_name: core-java
  content_path: book_content/core_java_content.txt

- book_name: spring-in-action
  content_path: book_content/spring_in_action_content.txt
```

---

### 2. Execute retrieval pipeline

From the project root:

```bash
python -m feature_achievement.run_retrieval
```

This will:

1. Load and normalize all configured documents
2. Construct chapter-level text signals
3. Build inverted indices for candidate generation
4. Compute TF-IDF similarity scores
5. Generate chapter-to-chapter relationship edges

Edges are printed to stdout and can be redirected or persisted as needed.

---

## ðŸ”— Edge Format

```json
{
  "from": "core-java::ch3",
  "to": "spring-in-action::ch1",
  "type": "tfidf_similarity",
  "score": 0.37
}
```

* `from` / `to`: Chapter identifiers
* `type`: Relationship type
* `score`: Continuous similarity score (TF-IDF)

---

## ðŸ§  Design Rationale

* **Pipeline before models**
  The system prioritizes observability and debuggability over early use of opaque models.

* **Candidate generation â‰  similarity scoring**
  Retrieval stages are explicitly separated to control complexity and scaling behavior.

* **Stop at the right abstraction boundary**
  Embeddings and vector databases are recognized as the natural next step, but intentionally excluded to keep the project realistic, inspectable, and aligned with intern-level ownership.

* **Interfaces over hard-coded logic**
  Core components are abstracted to enable future extensions without structural rewrites.

---

## ðŸš€ Intended Extensions (Out of Scope for Current Version)

* Embedding-based similarity (sentence transformers)
* Vector indices (FAISS / HNSW)
* FastAPI service layer for internal tooling
* Batch processing via message queues
* Graph visualization frontend

These are treated as **future evolution paths**, not missing features.

---

## ðŸ›  Tech Stack

* Python 3
* scikit-learn (TF-IDF)
* YAML-based configuration
* Modular, retrieval-oriented pipeline design

---

## ðŸ“Œ Project Status

This project intentionally concludes at a **retrieval-focused, pre-embedding stage** and currently emphasizes:

* Correctness of ingestion and normalization
* Clear separation of retrieval stages
* Stable and explainable system behavior
* Sound engineering judgment over feature breadth